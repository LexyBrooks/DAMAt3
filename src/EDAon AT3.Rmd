---
title: "EDA on AT3: Text Mining"
author: "Alex Brooks"
date: "10/14/2018"
output: html_document
---

```{r setup, include=FALSE}

library(tm) 
library(lsa)
library(SnowballC)
library(ggplot2)
library(wordcloud)
library(cluster)

```

##EDA on docs

Looking at docs corpus and analysing themes.

```{r}
#Let's go. Load corpus... 
docs <- VCorpus(DirSource("../raw_data/doc_corpus"))

num.docs <- length(docs)
```

#Write a standard function to clean the documents
```{r}

clean_docs <- function(docs) {
  
  output = docs
  
  #Remove punctuation - replace punctuation marks with " "
  output = tm_map(output, removePunctuation)
  
  #Transform to lower case
  output = tm_map(output,content_transformer(tolower))
  
  #Strip digits
  output = tm_map(output, removeNumbers)
  
  #Remove stopwords from standard stopword list 
  output = tm_map(output, removeWords, stopwords("english"))
  
  #Create a custom set of stopwords that weren't removed by the standard stopwords list
  myStopwords <- c("can",
                   "also",
                   "get",
                   "see",
                   "may",
                   "much",
                   "now",
                   "said",
                   "will",
                   "way",
                   "well",
                   "howev",
                   "say",
                   "one",
                   "use")
  
  #Remove the custom stopwords
  output = tm_map(output, removeWords, myStopwords)
  
  #Strip whitespace (cosmetic?)
  output = tm_map(output, stripWhitespace)
  
  return(output)
  
}
```

```{r}
clean.docs <- clean_docs(docs)
```

#A bit of basic analysis of documents with this function
```{r}

document_info <- function(documents, clean.documents) {
  
  num.documents = length(clean.documents)
  
  for(i in 1:num.documents) {
    document.meta = clean.documents[[i]]$meta
    document.content = clean.documents[[i]]$content
    raw.document.content = documents[[i]]$content
    
    if (i == 1) {
      document.summary = data.frame(id = document.meta$id, language = document.meta$language, datetimestamp = document.meta$datetimestamp, raw.characters = nchar(raw.document.content), clean.characters = nchar(document.content))
    } else {
      document.temp = data.frame(id = document.meta$id, language = document.meta$language, datetimestamp = document.meta$datetimestamp, raw.characters = nchar(raw.document.content), clean.characters = nchar(document.content))
      document.summary = rbind(document.summary, document.temp)
    }
  }
  
  return(document.summary)
}
```

```{r}
#running the function on raw docs and clean docs
document_info(docs, clean.docs)

```

##Insights on the corpus


```{r}
clean.docs <- tm_map(clean.docs,stemDocument)
#inspect
writeLines(as.character(clean.docs[[num.docs]]))
#end of preprocessing

```


```{r}
#Create document-term matrix
dtm <- DocumentTermMatrix(clean.docs)

#summary
dtm
#4442 terms in the 41 docs
#inspect segment of document term matrix
inspect(dtm[1:10,995:1006])
#collapse matrix by summing over columns - this gets total counts (over all docs) for each term
freq <- colSums(as.matrix(dtm))
#length should be total number of terms
length(freq)
#create sort order (asc)
ord <- order(freq,decreasing=TRUE)
#inspect most frequently occurring terms
freq[head(ord)]
#write to disk and inspect file
write.csv(file="../clean_data/freq.csv",freq[ord])

```

## Including Plots

```{r}
#inspect least frequently occurring terms
freq[tail(ord)]
#list most frequent terms. Lower bound specified as second argument
findFreqTerms(dtm,lowfreq=80)

```
You can also embed plots, for example:
```{r}
#correlations
findAssocs(dtm,"algorithm",0.9)
findAssocs(dtm,"argument",0.75)
findAssocs(dtm,"cluster",0.9)
findAssocs(dtm,"project",0.69)
findAssocs(dtm,"discuss",0.75)
findAssocs(dtm,"model",0.75)
findAssocs(dtm,"risk",0.7)
findAssocs(dtm,"question",0.7)
findAssocs(dtm,"document",0.9)
```


```{r}
#histogram
wf = data.frame(term=names(freq), occurrences=freq)

ggplot(subset(wf, occurrences>200), aes(term, occurrences)) + geom_bar(stat="identity") + theme(axis.text.x=element_text(angle=45, hjust=1))
```


```{r}
#order by frequency
ggplot(subset(wf, occurrences>200), aes(reorder(term,occurrences), occurrences)) + geom_bar(stat="identity") + theme(axis.text.x=element_text(angle=45, hjust=1)) 
```


```{r}
#wordcloud
#setting the same seed each time ensures consistent look across clouds
set.seed(42)

#limit words by specifying min frequency
wordcloud(names(freq),freq, max.words=40)

#...add color
wordcloud(names(freq),freq,max.words=40,colors=brewer.pal(6,"Dark2"))

```
#Making bigrams of words in the corpus

```{r}
#to see what ngrams does, try running ngrams(words(docs[[1]]$content),2), which
#returns bigrams for the first document in the corpus
BigramTokenizer <-  function(x) unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
#create DTM
dtmbi <- DocumentTermMatrix(clean.docs, control = list(tokenize = BigramTokenizer))
freqbi <- colSums(as.matrix(dtmbi))
#length should be total number of terms
length(freqbi)
#create sort order (asc)
ordbi <- order(freqbi,decreasing=TRUE)
#inspect most frequently occurring terms
freqbi[head(ordbi)]

```
##TF-IDF
```{r}
dtm_tfidf <- DocumentTermMatrix(clean.docs, control = list(weighting = weightTfIdf))
#note that the weighting is normalised by default (that is, the term frequencies in a
#document are normalised by the number of terms in the document)
#summary
dtm_tfidf
#inspect segment of document term matrix
inspect(dtm_tfidf[1:10,1000:1006])
#collapse matrix by summing over columns - this gets total weights (over all docs) for each term
wt_tot_tfidf <- colSums(as.matrix(dtm_tfidf))
#length should be total number of terms
length(wt_tot_tfidf )
#create sort order (asc)
ord_tfidf <- order(wt_tot_tfidf,decreasing=TRUE)
#inspect most frequently occurring terms
wt_tot_tfidf[head(ord_tfidf)]
#write to disk and inspect file
write.csv(file="../clean_data/wt_tot_tfidf.csv",wt_tot_tfidf[ord_tfidf])
#inspect least weighted terms
wt_tot_tfidf[tail(ord_tfidf)]
```
```{r}
#correlations - compare to dtm generated by  tf and tf/truncated weighting
#"project" at correlation level of 0.6
findAssocs(dtm_tfidf,"risk",0.5)
findAssocs(dtm_tfidf,"distribution",0.8)
findAssocs(dtm_tfidf,"eleph",0.8)
findAssocs(dtm_tfidf,"wickham",0.8)
findAssocs(dtm_tfidf,"hubbard",0.8)
findAssocs(dtm_tfidf,"scapegoat",0.8)
#notice the difference!
```
```{r, fig.width=10, fig.height=12}
#histogram
wf=data.frame(term=names(wt_tot_tfidf),weights=wt_tot_tfidf)
#library(ggplot2)
ggplot(subset(wf, wt_tot_tfidf>.1), aes(reorder(term,weights), weights)) + geom_bar(stat="identity") + theme(axis.text.x=element_text(angle=45, hjust=1))
```
#WOrdcloud of TF-IDF - to see weighted words
```{r}
#wordcloud
#library(wordcloud)
#setting the same seed each time ensures consistent look across clouds
set.seed(42)
#limit words by specifying min total wt
wordcloud(names(wt_tot_tfidf),wt_tot_tfidf, max.words=100)
#...add color
wordcloud(names(wt_tot_tfidf),wt_tot_tfidf,max.words=100,colors=brewer.pal(6,"Dark2"))
#play with different values of max.words
#try specifying min.freq instead of max.words
```
##Heirarchical clustering
```{r}
#Create document-term matrix
cluster <- DocumentTermMatrix(clean.docs)
## start clustering specific code
#convert dtm to matrix (what format is the dtm stored in?)
cluster.matrix <- as.matrix(cluster)
#write as csv file
write.csv(cluster.matrix,file="../clean_data/ClusterAsMatrix.csv")
```


```{r}
#shorten rownames for display purposes
#rownames(m_cluster) <- paste(substring(rownames(m),1,3),rep("..",nrow(m)),
                    #substring(rownames(m_cluster),
                    #nchar(rownames(m_cluster))-12,nchar(rownames(m_cluster))-4))
#compute distance between document vectors
hcluster.distance <- dist(cluster.matrix, method="euclidean")
#run hierarchical clustering using Ward's method (explore other options later)
hcluster.groups <- hclust(hcluster.distance,method="ward.D")
#plot, use hang to ensure that labels fall below tree
plot(hcluster.groups, hang=-1)
#cut into 2 subtrees. Try 3,4,5,6 cuts; comment on your results
rect.hclust(hcluster.groups,2)
hcluster.hclusters <- cutree(hcluster.groups,2)
write.csv(hcluster.hclusters,"../clean_data/hclusters.csv")
```
##Cosine distance clusters
```{r}


##try another distance measure
cosineSim <- function(x){
  as.dist(x%*%t(x)/(sqrt(rowSums(x^2) %*% t(rowSums(x^2)))))
}
cosine <- cosineSim(cluster.matrix)
cosine.distance <- cosine

#run hierarchical clustering using cosine distance
cosine.groups <- hclust(cosine.distance, method="ward.D")
#plot, use hang to ensure that labels fall below tree
plot(cosine.groups, hang=-1)
#cut into 2 subtrees.
rect.hclust(cosine.groups, 2)
cosine.hclusters <- cutree(cosine.groups, 2)
write.csv(cosine.hclusters, "../clean_data/hclusters_cosine.csv")

```
#KMeans clustering
```{r}
#kmeans clustering
#kmeans - run with nstart=100 and k=2,3,5 to compare results with hclust
kfit <- kmeans(hcluster.distance, centers=3, nstart=100)
#plot - need library cluster
clusplot(as.matrix(hcluster.distance), kfit$cluster, color=T, shade=T, labels=2, lines=0)
#print contents of kfit
print(kfit)
#print cluster sizes
kfit$size
#print clusters (members)
kfit$cluster
#write clusters to csv file
write.csv(kfit$cluster,file="../clean_data/KMClustGroups2.csv")
#sum of squared distance between cluster centers 
kfit$betweenss
#sum of squared distance within a cluster (this are the quantities that the algorithm
#attempts to minimise)
kfit$withinss

#kmeans - how to determine optimal number of clusters?

#One approach: look for "elbow" in plot of summed intra-cluster distances (withinss) as fn of k

wss <- 2:(length(clean.docs)-1)

for (i in 2:(length(clean.docs)-1)) {
  wss[i] <- sum(kmeans(hcluster.distance,centers=i,nstart=25)$withinss)
}

plot(2:(length(clean.docs)-1), wss[2:(length(clean.docs)-1)], type="b", xlab="Number of Clusters", ylab="Within groups sum of squares") 
```

##Igraph
```{r}
library(igraph)
m<-as.matrix(dtm)

#Map filenames to matrix row numbers
#these numbers will be used to reference files in the network graph
filekey <- cbind(1:length(docs),rownames(m))
write.csv(filekey,"filekey.csv",row.names = FALSE)
#have a look at file
rownames(m) <- 1:length(docs)
#compute cosine similarity between document vectors
#converting to distance matrix sets diagonal elements to 0
cosineSim <- function(x){
  as.dist(x%*%t(x)/(sqrt(rowSums(x^2) %*% t(rowSums(x^2)))))
}
cs <- cosineSim(m)

#adjacency matrix: set entries below a certain threshold to 0.
#We choose half the magnitude of the largest element of the matrix
#as the cutoff. This is an arbitrary choice
cs[cs < max(cs)/2] <- 0
cs <- round(cs,3)
#write to disk
write.csv(as.matrix(cs),file="../clean_data/AdjacencyMatrix.csv")
#open it and have a look
```
#build that igraph
```{r, fig.width=10, fig.height=12}
# build a graph from the above matrix
#mode is undirected because similarity is a bidirectional relationship
igraph <- graph.adjacency(as.matrix(cs), weighted=T, mode = "undirected")
#Plot a Graph
# set seed to make the layout reproducible
set.seed(42)
#one of many possible layouts, see igraph docs
layout1 <- layout.fruchterman.reingold(igraph)
#basic plot with no weighting - fruchtermann reingold weighting
plot(igraph, layout=layout1)
#another layout
plot(igraph, layout=layout.kamada.kawai)
# 19,20,21,22 all related
#24,23,18,27,31,34,24 all sit out on their own
#33 and 26 relate back to 25, which relates back to 9 and 29
#36 and 37 relate back to 41, which has other relations
```
#fast.greedy
```{r, fig.width=10, fig.height=12}
#Community detection - Fast/Greedy
comm_fg <- fastgreedy.community(igraph)
comm_fg$membership
V(igraph)$color <- comm_fg$membership
plot(igraph, layout=layout.kamada.kawai)
community_mapping <- cbind(as.data.frame(filekey, row.names = F),comm_fg$membership)
community_mapping
#24, 18 and 31 are their own community
#34 is similar to 38 and the other greens
```
#Louvain
```{r, fig.width=10, fig.height=12}
#Community detection - Louvain
comm_lv <- cluster_louvain(igraph)
comm_lv$membership
V(igraph)$color <- comm_lv$membership
plot(igraph, layout=layout.kamada.kawai)
community_mapping <- cbind(community_mapping,comm_lv$membership)
community_mapping
#it sees the previous yellow as the same as the orange, but sees 34 as relating to 14, 17 etc
#slightly different communities
```
#another way with Network Graphs
```{r, fig.width=10, fig.height=12}
#lets weight the nodes and edges
#set label (not really necessary)
#V=vertex, E=edge
V(igraph)$label <- V(igraph)$name
#Vertex size proportional to number of connections
V(igraph)$size <- degree(igraph)*.6
#Vertex label size proportional to number of connections
V(igraph)$label.cex <-  degree(igraph) / max(degree(igraph))+ .8
#label colour default black
V(igraph)$label.color <- "black"
#Vertex color organe
V(igraph)$color <- "orange"
#edge color grey
E(igraph)$color <- "grey"
#edge width proportional to similarity (weight)
E(igraph)$width <- E(igraph)$weight*7
# plot the graph in layout1 (fruchtermann reingold)
plot(igraph, layout=layout1)
#output is quite ugly. Explore igraph to see how you
#can fix it
#9 is important, so is 6 and 7
```
```{r, fig.width=10, fig.height=12}
#lets weight the nodes and edges
#set label (not really necessary)
#V=vertex, E=edge
V(igraph)$label <- V(igraph)$name
#Vertex size proportional to number of connections
V(igraph)$size <- degree(igraph)*.6
#Vertex label size proportional to number of connections
V(igraph)$label.cex <-  degree(igraph) / max(degree(igraph))+ .6
#label colour default black
V(igraph)$label.color <- "black"
#Vertex color organe
V(igraph)$color <- "orange"
#edge color grey
E(igraph)$color <- "grey"
#edge width proportional to similarity (weight)
E(igraph)$width <- E(igraph)$weight*5
# plot the graph in layout1 (fruchtermann reingold)
plot(igraph, layout=layout.auto)
#output is quite ugly. Explore igraph to see how you
#can fix it
plot(igraph, layout=layout1)
```
#LSA
```{r}
library(lsa)
#Create term-document matrix (lsa expects a TDM rather than a DTM)
tdm <- TermDocumentMatrix(clean.docs)
#summary
tdm
#inspect segment of document term matrix
inspect(tdm[1000:1006,1:10])

#what kind of object is the tdm?
class(tdm)
#note: a simple triplet matrix (STM) is an efficient format to store
#sparse matrices
#need to convert STM to regular matrix
#convert to regular matrix
tdm.matrix <- as.matrix(tdm)
#check class
class(tdm.matrix)
dim(tdm.matrix)
```

```{r}
#weight terms and docs

#Note: We weight the TDM before calculating the latent semantic space.
#This is to better reflect the relative importance of each term/doc 
#in relation to the entire corpus (much like tf-idf weighting).  
#It is convenient to express the transformation as a product of 
#two numbers - local and global weight functions, like so:
#a (i,j) = L(i,j)*G(i).
#The local weight function L(i,j) presents the weight of term i 
#in document j. The global weight function G(i) is used to express 
#the weight of the term iacross the entire document set. 

#We'll use the equivalent of tf-idf (local weight - tf, global -idf)
#check out other weighting schemes in the documentation (link above)
tdm.matrix.lsa <- lw_tf(tdm.matrix) * gw_idf(tdm.matrix)
dim(tdm.matrix.lsa)

```
```{r}
#compute the Latent semantic space
lsaSpace <- lsa(tdm.matrix.lsa, dimcalc_share()) # create LSA space
#examine output
names(lsaSpace)
```
```{r}
#Original Matrix is decomposed as: 
#tk(nterms,lsadim).Sk(lsadim).dk*(lsadim,ndocs)
#where 
#nterms=number of terms in TDM
#ndocs=number of docs in TDM
#lsadim=dimensionality of Latent Semantic Space (length of Sk)
LSAMat <- as.textmatrix(lsaSpace)
#1)
#Examine a term in LS space
LSAMat["social",1:10]
#compare to Term-frequency space
tdm.matrix.lsa["social",1:10]
#try other words
#What does this tell you about the term vectors in the two spaces?

```
