---
title: "So my boss just handed me a load of documents"
author: "Alex Brooks"
date: "10/14/2018"
output: html_document
---
##My goal is to find out what's in these 41 different text documents
```{r setup, include=FALSE}
library(tm) 
library(lsa)
library(SnowballC)
library(ggplot2)
library(wordcloud)
library(cluster)
library(dplyr)      #manipulate dataframes
library(knitr)       # used to make kable tables
library(magrittr)
library(ggthemes)    #to make plots look more interesting
library(topicmodels)
library(tidytext)
library(tidyr)
library(kableExtra)
library(igraph)
library(ggraph)
library(widyr)
```

##Hey boss, here's how I started to take a look at those 41 different documents

Looking at docs corpus and analysing themes means loading the docs and cleaning them to turn them into the correct format for analysis.

```{r}
#Load the corpus
docs <- VCorpus(DirSource("../raw_data/doc_corpus"))
```

#Write a standard function to clean the documents
```{r}

clean_docs <- function(docs) {
  
  output = docs
  
  #Remove punctuation - replace punctuation marks with " "
  output = tm_map(output, removePunctuation)
  
  #Transform to lower case
  output = tm_map(output,content_transformer(tolower))
  
  #Strip digits
  output = tm_map(output, removeNumbers)
  
  #Remove stopwords from standard stopword list 
  output = tm_map(output, removeWords, stopwords("english"))
  
  #Create a custom set of stopwords that weren't removed by the standard stopwords list
  myStopwords <- c("can",
                   "also",
                   "get",
                   "see",
                   "may",
                   "much",
                   "now",
                   "said",
                   "will",
                   "way",
                   "well",
                   "howev",
                   "say",
                   "one",
                   "use",
                   "tm_map",
                   "t_")
  
  #Remove the custom stopwords
  output = tm_map(output, removeWords, myStopwords)
  
  #Strip whitespace (cosmetic?)
  output = tm_map(output, stripWhitespace)
  
  return(output)
  
}
```

```{r}
clean.docs <- clean_docs(docs)
```

#A bit of basic analysis of documents with this function
```{r}

document_info <- function(documents, clean.documents) {
  
  num.documents = length(clean.documents)
  
  for(i in 1:num.documents) {
    document.meta = clean.documents[[i]]$meta
    document.content = clean.documents[[i]]$content
    raw.document.content = documents[[i]]$content
    
    if (i == 1) {
      document.summary = data.frame(id = document.meta$id, language = document.meta$language, datetimestamp = document.meta$datetimestamp, raw.characters = nchar(raw.document.content), clean.characters = nchar(document.content))
    } else {
      document.temp = data.frame(id = document.meta$id, language = document.meta$language, datetimestamp = document.meta$datetimestamp, raw.characters = nchar(raw.document.content), clean.characters = nchar(document.content))
      document.summary = rbind(document.summary, document.temp)
    }
  }
  
  return(document.summary)
}
```

```{r}

document_table <- function(documents) {
  for (i in 1:length(documents)) {
    document.meta = documents[[i]]$meta
    document.content = documents[[i]]$content
    if (i == 1) {
      document.summary = data.frame(id = document.meta$id, text = document.content)
    } else {
      document.temp = data.frame(id = document.meta$id, text = document.content)
      document.summary = rbind(document.summary, document.temp)
    }
  }
  return(document.summary)
}
```

```{r}
#running the function on raw docs and clean docs
document_info(docs, clean.docs)
#shortest doc is just 5072 raw characters, longest is doc19 has raw 34921
#ANT: I want to plot this
##Also, I want to know if there's a way to 'rename' the document according to it's theme - IE, can I examine each document singularly and take a look at each one??
#I apparently need to turn it into a dataframe for plotting so I tried the following code, but it wouldn't work
#data.frame(text = sapply(clean.docs, as.character), stringsAsFactors = FALSE)
#plot length of clean characters in each doc
#ggplot(data.frame, aes(id, clean.characters)) + geom_bar(stat = "identity",fill='lightblue',color='black') + theme(axis.text.x = element_text(angle = 45, hjust = 1)) +  theme_economist()+scale_color_economist() 
```
 
##Insights on the corpus

```{r}
clean.docs <- tm_map(clean.docs,stemDocument)
#inspect
writeLines(as.character(clean.docs[[num.docs]]))
#end of preprocessing

```

```{r}
#Create document-term matrix
dtm <- DocumentTermMatrix(clean.docs)
#summary
dtm
```

There are 41 separate documents using 4492 terms with the term frequency weighting. The maximal term length is 114 - that's long! It could be an error
#4492 terms in the 41 docs
```{r}
#inspect segment of document term matrix
inspect(dtm[15:20,3200:3492])
#This just gives a sample, but I checked various numbers and the terms very much relate to the themes of machine learning, data science, risk management, project management and monte carlo probabilities.
#collapse matrix by summing over columns - this gets total counts (over all docs) for each term
freq <- colSums(as.matrix(dtm))
freq
#also academi terms, and appendix
#length should be total number of terms
length(freq)
#create sort order (asc)
ord <- order(freq,decreasing=TRUE)
ord
#inspect most frequently occurring terms
freq[head(ord)]
#write to disk and inspect file ##ANT THIS CSV IS ACTUALLY EMPTY?
```


## Let's start visualising these themes after looking at the least frequent terms and then finding correlations with the frequent terms
Project, risk and management are the most frequent. What else could be interesting, especially from a data science perspective:
Algorithm, approach, cluster.
Author, article, knowledge, academi, appendix - seem very academic or authorly and could be of interest to Smartbox
Method, model, function, interest - relates to data science
Manage, organise, project, risk - relate to managing technically innovative projects that may relate to data science
Technique, term, understand, 
Word, post and work - the word "post" can mean letters but also posting on a blog or watching your post
```{r}
#inspect least frequently occurring terms - this isn't particularly helpful
freq[tail(ord)]
#list most frequent terms. Lower bound specified as second argument
OtherFreq <- findFreqTerms(dtm,lowfreq=30)
OtherFreq
```

#Finding correlations with the frequent terms
```{r}
#correlations
findAssocs(dtm,"algorithm",0.9)
#this doesn't look all that interesting - it's frequent, but correlations not giving information
```
```{r}
findAssocs(dtm,"cluster",0.75) 
#ANT I WANT TO PLOT THIS, BUT CAN'T DO ITclearly lots of associations, meaning this is relating to clustering in data science
#wordcloud of this could be good for the main report back to my boss
#setting the same seed each time ensures consistent look across clouds
#set.seed(42)
#wordcloud(names(dtm, "cluster", 0.75),dtm,max.words=50,colors=brewer.pal(6,"Dark2"))
#play with different values of max.words
#try specifying min.freq instead of max.words
```
```{r}
findAssocs(dtm,"project",0.6)
#projects in a management or business context. Lose dollar or win also has some kind of business or probability element
```
```{r}
findAssocs(dtm,"risk",0.5)
```


```{r}
#the associations with the term document reveal the corpus has plenty of information about text mining and R programs in it, with lots of snippets from cod like "setwduserskailahdocumentstextmin" and "tmmapsdocsstemdocu"
findAssocs(dtm,"document",0.8)
```
```{r}
findAssocs(dtm,"method",0.6)%>%
   kable()  
#reveals academic terms
```
```{r}
findAssocs(dtm,"time",0.8)%>%
   kable()  %>%
   kable_styling(bootstrap_options = "striped", full_width = F, font_size = 18) 
```

```{r}
#reveals an important component of risk and project management
findAssocs(dtm,"data",0.8)
findAssocs(dtm,"manag",0.8)
```
```{r, fig.width=10, fig.height=12}
findAssocs(dtm,"eleph", 0.8) %>%
   kable()  %>%
  kable_styling(bootstrap_options = "striped", full_width = F, font_size = 18) 
```


```{r}
#histogram
wf = data.frame(term=names(freq), occurrences=freq)

ggplot(subset(wf, occurrences>100), aes(term, occurrences)) + geom_bar(stat="identity", fill = 'lightblue', color='black') + theme(axis.text.x=element_text(angle=45, hjust=1)) + labs(title = "Most frequently occuring terms in the 41 documents",
             x = "Words", y = "occurences")
```
# Project is a word likely to co-occur with both management and other words like work or problem or example

```{r}
#order and condense the frequency words
ggplot(subset(wf, occurrences>200), aes(reorder(term,occurrences), occurrences)) + geom_bar(stat="identity", fill = 'lightblue', color='black') + theme(axis.text.x=element_text(angle=45, hjust=1, size=14, face="bold")) + labs(title = "Most frequently occuring terms in order of volume in 41 documents",
             x = "Words", y = "occurences")
```
 

```{r, fig.width=10, fig.height=12}
#wordcloud
#setting the same seed each time ensures consistent look across clouds
set.seed(42)

#limit words by specifying min frequency
wordcloud(names(freq),freq, max.words=80)

#...add color
wordcloud(names(freq),freq,max.words=75,colors=brewer.pal(6,"Dark2"))

```
#Make ngrams when you want to explore what's in a particular document
```{r, fig.width=20, fig.height=5}
#to see what ngrams does 
ngrams(words(docs[[1]]$content),2) %>%
  kable()  %>%
  kable_styling(bootstrap_options = "striped", full_width = F, font = 18) 
```
 

#Making bigrams of words in the corpus

```{r}

#returns bigrams for the first document in the corpus? OR IS THIS ALL THE DOCUMENTS/HOW CAN I TELL IN THE CODE? IF I CAN DO INDIVIDUAL DOCS, THAT COULD BE INTERESTING
BigramTokenizer <-  function(x) unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
#create DTM
dtmbi <- DocumentTermMatrix(clean.docs, control = list(tokenize = BigramTokenizer))
freqbi <- colSums(as.matrix(dtmbi))
#length should be total number of terms - seem to be 34967 bigrams
length(freqbi)
#create sort order (asc)
ordbi <- order(freqbi,decreasing=TRUE) 
#inspect most frequently occurring terms
freqbi[head(ordbi)] %>%
  kable()
#ANT QUESTION - CAN I PLOT THIS OR MAKE A TABLE LIKE I DID ABOVE?

```


```{r}

document.text <- document_table(docs)
document.text
```

```{r}

#trying with tidytext
corpus_bigrams <- document.text %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)
corpus_bigrams
```
```{r}
#counting bigrams
corpus_bigrams %>%
  count(bigram, sort = TRUE)
bigrams_separated <- corpus_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts <- bigrams_filtered %>%
  count(word1, word2, sort = TRUE)

bigram_counts
```
```{r}
#looking at risk and management and project management bigrams
bigrams_filtered %>%
  filter(word2 == "risk") %>%
  count(id, word1, sort = TRUE)
```
```{r}
bigram_graph <- bigram_counts %>%
  filter(n > 20) %>%
  graph_from_data_frame()

bigram_graph

```
```{r}
set.seed(42)
ggraph(bigram_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)
```
```{r, fig.width=12}
#improving the visual
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void() + labs(title = "Most frequently occuring bigrams")
```


##TF-IDF
TF is the term frequency, or how frequently a word occurs in a document. IDF is inverse document frequency, which decreases the weight for commonly used words and increases the weight for words that are not used very much in a collection of documents. 
```{r}
#tidytext approach first - which documents does management appear in? Only 25 of the 41 docs
bigrams_filtered %>%
  filter(word1 == "management") %>%
  count(id, word1, sort = TRUE)%>%
  kable()  %>%
  kable_styling(bootstrap_options = "striped", full_width = F, font = 18) 
```

 
```{r}
bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

bigrams_united %>%
  filter(bigram =="project management") %>%
  kable()  %>%
  kable_styling(bootstrap_options = "striped", full_width = F, font = 18) 
```
```{r}
bigrams_united %>%
  filter(bigram =="risk management") %>%
  kable()  %>%
  kable_styling(bootstrap_options = "striped", full_width = F, font = 18) 
```
```{r}
bigrams_united %>%
  filter(bigram =="monte carlo") %>%
  kable()  %>%
  kable_styling(bootstrap_options = "striped", full_width = F, font = 18) 
```


```{r}
dtm_tfidf <- DocumentTermMatrix(clean.docs, control = list(weighting = weightTfIdf))
#note that the weighting is normalised by default (that is, the term frequencies in a
#document are normalised by the number of terms in the document)
#summary
dtm_tfidf

#inspect segment of document term matrix
inspect(dtm_tfidf[01:10,2000:2006])

```

```{r}
#collapse matrix by summing over columns - this gets total weights (over all docs) for each term
wt_tot_tfidf <- colSums(as.matrix(dtm_tfidf))
#length should be total number of terms
length(wt_tot_tfidf )
#create sort order (asc)
ord_tfidf <- order(wt_tot_tfidf,decreasing=TRUE)
#inspect most frequently occurring terms
wt_tot_tfidf[head(ord_tfidf)] %>% 
  kable()
#write to disk and inspect file
write.csv(file="../clean_data/wt_tot_tfidf.csv",wt_tot_tfidf[ord_tfidf])
```
```{r}
#inspect least weighted terms - wickham, wordcloudnames and what looks like a jumble of r code snippets. youtube, zip
wt_tot_tfidf[tail(ord_tfidf)] %>%
  kable()
```

```{r}
#correlations - compare to dtm generated by  tf and tf/truncated weighting "project" at correlation level of 0.6
findAssocs(dtm_tfidf,"risk",0.5) %>%
  kable()
```


```{r}
findAssocs(dtm_tfidf,"eleph",0.6)
#HOW CAN I MAKE A WORD CLOUD OF THIS PARTICULAR ONE
```
```{r}
findAssocs(dtm_tfidf,"monte carlo",0.8)
```
```{r}
findAssocs(dtm_tfidf,"wickham",0.8)
```
```{r}
findAssocs(dtm_tfidf,"risk",0.4)
```

```{r}
findAssocs(dtm_tfidf,"post",0.5)
```

```{r}
findAssocs(dtm_tfidf,"project",0.5)
findAssocs(dtm_tfidf,"manag",0.5)
```

```{r, fig.width=10, fig.height=12}
#histogram
wf=data.frame(term=names(wt_tot_tfidf),weights=wt_tot_tfidf)
#library(ggplot2)
ggplot(subset(wf, wt_tot_tfidf>.1), aes(reorder(term,weights), weights)) + geom_bar(stat="identity", fill = 'lightblue', color='black') + theme(axis.text.x=element_text(angle=45, hjust=1, size=14, face="bold")) + labs(title = "Tf-idf terms in all docs", x = "Words", y = "tf-idf weights") 

```
#Wordcloud of TF-IDF - to see weighted words
```{r, fig.width=15, fig.height=12}
#wordcloud
#setting the same seed each time ensures consistent look across clouds
set.seed(42)
#limit words by specifying min total wt
wordcloud(names(wt_tot_tfidf),wt_tot_tfidf, max.words=100)
#...add color
wordcloud(names(wt_tot_tfidf),wt_tot_tfidf,max.words=100,colors=brewer.pal(6,"Dark2"))
#play with different values of max.words
#try specifying min.freq instead of max.words
wordcloud(names(wt_tot_tfidf),wt_tot_tfidf,min.freq=50,colors=brewer.pal(6,"Dark2"))



```
##Cluster analysis
What can we discover. Clustering should allow us to see what 'goes' together - can our three themes of "monte carlo", "data science and machine learning" and "project and risk management" be identified?

We need to report on the different results of the clustering and justify what we went with.

This is a good tutorial on doing all the clustering and making one "win" https://recast.ai/blog/text-clustering-with-r-an-introduction-for-data-scientists/ = would be good to replicate this for this assignment. This is Kailash's post about it https://eight2late.wordpress.com/2015/07/22/a-gentle-introduction-to-cluster-analysis-using-r/
```{r}
#Create document-term matrix
cluster <- DocumentTermMatrix(clean.docs)
## start clustering specific code
#convert dtm to matrix  
cluster.matrix <- as.matrix(cluster)
#write as csv file
write.csv(cluster.matrix,file="../clean_data/ClusterAsMatrix.csv")
```


```{r}
#ward D with 3 hangs
hcluster.distance <- dist(cluster.matrix, method="euclidean")
#run hierarchical clustering using Ward's method (explore other options later)
hcluster.groups <- hclust(hcluster.distance,method= 'ward.D')
#plot 
plot(hcluster.groups, hang=-3)
#ASK ANT ABOUT RECT ERROR IN ALL CLUSTERING
```
#This gives two big splits,clustering19-22 and 35, 38,39,40 and 41 together
```{r}
#wardD2 with 5 hangs
hcluster.groups2 <- hclust(hcluster.distance,method= 'ward.D2')
#plot 
plot(hcluster.groups2, hang=-5)
hcluster.hclusters <- cutree(hcluster.groups,2)
write.csv(hcluster.hclusters,"../clean_data/hclusterswardd2.csv")
#ASK ANT ABOUT RECT ERROR IN ALL CLUSTERING
```
 
##Cosine distance clusters
```{r}


##try another distance measure
cosineSim <- function(x){
  as.dist(x%*%t(x)/(sqrt(rowSums(x^2) %*% t(rowSums(x^2)))))
}
cosine <- cosineSim(cluster.matrix)
cosine.distance <- cosine

#run hierarchical clustering using cosine distance
cosine.groups <- hclust(cosine.distance, method="ward.D")
#plot, use hang to ensure that labels fall below tree
plot(cosine.groups, hang=-1)
#cut into 2 subtrees.
rect.hclust(cosine.groups, 3)
cosine.hclusters <- cutree(cosine.groups, 3)
write.csv(cosine.hclusters, "../clean_data/hclusters_cosine.csv")
#ASK ANT ABOUT RECT ERROR IN CLUSTERING BUT NOT IN THIS ONE - I WOULD NEED TO PLAY TO GET THIS TO SHOW WHAT I WANT IT TO SHOW!!! BUT IT WOULD BE GOOD TO HAVE ONE OF THESE VISUALISATIONS

```
#KMeans clustering
```{r, fig.width=15, fig.height=15}
#kmeans clustering
#kmeans - run with nstart=100 and k=2,3,5 to compare results with hclust
kfit <- kmeans(hcluster.distance, centers=3, nstart=200)
#plot - need library cluster
clusplot(as.matrix(hcluster.distance), kfit$cluster, color=T, shade=T, labels=2, lines=0)
#print contents of kfit
print(kfit)
#print cluster sizes
kfit$size
#print clusters (members)
kfit$cluster
#write clusters to csv file
write.csv(kfit$cluster,file="../clean_data/KMClustGroups2.csv")
#sum of squared distance between cluster centers 
kfit$betweenss
#sum of squared distance within a cluster (this are the quantities that the algorithm
#attempts to minimise)
kfit$withinss

#kmeans - how to determine optimal number of clusters?

#One approach: look for "elbow" in plot of summed intra-cluster distances (withinss) as fn of k

wss <- 2:(length(clean.docs)-1)

for (i in 2:(length(clean.docs)-1)) {
  wss[i] <- sum(kmeans(hcluster.distance,centers=i,nstart=25)$withinss)
}

plot(2:(length(clean.docs)-1), wss[2:(length(clean.docs)-1)], type="b", xlab="Number of Clusters", ylab="Within groups sum of squares") 
```

##Igraph
```{r}
library(igraph)
m<-as.matrix(dtm)

#Map filenames to matrix row numbers
#these numbers will be used to reference files in the network graph
filekey <- cbind(1:length(docs),rownames(m))
write.csv(filekey,"filekey.csv",row.names = FALSE)
#have a look at file
rownames(m) <- 1:length(docs)
#compute cosine similarity between document vectors
#converting to distance matrix sets diagonal elements to 0
cosineSim <- function(x){
  as.dist(x%*%t(x)/(sqrt(rowSums(x^2) %*% t(rowSums(x^2)))))
}
cs <- cosineSim(m)

#adjacency matrix: set entries below a certain threshold to 0.
#We choose half the magnitude of the largest element of the matrix
#as the cutoff. This is an arbitrary choice
cs[cs < max(cs)/2] <- 0
cs <- round(cs,3)
#write to disk
write.csv(as.matrix(cs),file="../clean_data/AdjacencyMatrix.csv")
#open it and have a look
cosineSim
```
#build that igraph
I need to understand how this works better - I can't seem to find the original COmmunity Graph I built??? http://rpubs.com/LexyBrooks/432095
```{r, fig.width=10, fig.height=12}
# build a graph from the above matrix
#mode is undirected because similarity is a bidirectional relationship
igraph <- graph.adjacency(as.matrix(cs), weighted=T, mode = "undirected")
#Plot a Graph
# set seed to make the layout reproducible
set.seed(42)
#one of many possible layouts, see igraph docs
layout1 <- layout.fruchterman.reingold(igraph)
#basic plot with no weighting - fruchtermann reingold weighting
plot(igraph, layout=layout1)
#another layout
plot(igraph, layout=layout.kamada.kawai)
# 19,20,21,22 all related
#24,23,18,27,31,34,24 all sit out on their own
#33 and 26 relate back to 25, which relates back to 9 and 29
#36 and 37 relate back to 41, which has other relations
```
#fast.greedy 
```{r, fig.width=10, fig.height=12}
#Community detection - Fast/Greedy
comm_fg <- fastgreedy.community(igraph)
comm_fg$membership
V(igraph)$color <- comm_fg$membership
plot(igraph, layout=layout.kamada.kawai)
community_mapping <- cbind(as.data.frame(filekey, row.names = F),comm_fg$membership)
community_mapping
#24, 18 and 31 are their own community
#34 is similar to 38 and the other greens
```
#Louvain
Community Map - shows the significance of the three thematic areas I've chosen to look at (data science/R, project/risk management, monte carlo)
```{r, fig.width=10, fig.height=12}
#Community detection - Louvain
comm_lv <- cluster_louvain(igraph)
comm_lv$membership
V(igraph)$color <- comm_lv$membership
plot(igraph, layout=layout.kamada.kawai)
community_mapping <- cbind(community_mapping,comm_lv$membership)
community_mapping
#it sees the previous yellow as the same as the orange, but sees 34 as relating to 14, 17 etc
#slightly different communities
```
#another way with Network Graphs
```{r, fig.width=10, fig.height=12}
#lets weight the nodes and edges
#set label (not really necessary)
#V=vertex, E=edge
V(igraph)$label <- V(igraph)$name
#Vertex size proportional to number of connections
V(igraph)$size <- degree(igraph)*.6
#Vertex label size proportional to number of connections
V(igraph)$label.cex <-  degree(igraph) / max(degree(igraph))+ .8
#label colour default black
V(igraph)$label.color <- "black"
#Vertex color organe
V(igraph)$color <- "orange"
#edge color grey
E(igraph)$color <- "grey"
#edge width proportional to similarity (weight)
E(igraph)$width <- E(igraph)$weight*7
# plot the graph in layout1 (fruchtermann reingold)
plot(igraph, layout=layout1)
#output is quite ugly. Explore igraph to see how you
#can fix it
#9 is important, so is 6 and 7
```
```{r, fig.width=10, fig.height=12}
#lets weight the nodes and edges
#set label (not really necessary)
#V=vertex, E=edge
V(igraph)$label <- V(igraph)$name
#Vertex size proportional to number of connections
V(igraph)$size <- degree(igraph)*.6
#Vertex label size proportional to number of connections
V(igraph)$label.cex <-  degree(igraph) / max(degree(igraph))+ .6
#label colour default black
V(igraph)$label.color <- "black"
#Vertex color organe
V(igraph)$color <- "orange"
#edge color grey
E(igraph)$color <- "grey"
#edge width proportional to similarity (weight)
E(igraph)$width <- E(igraph)$weight*5
# plot the graph in layout1 (fruchtermann reingold)
plot(igraph, layout=layout.auto)
#output is quite ugly. Explore igraph to see how you
#can fix it
plot(igraph, layout=layout1)
```
#LSA
Also called latent semantic analysis (LSA), uncovers the underlying latent semantic structure in the usage of words in a body of text and how it can be used to extract the meaning of the text in response to user queries, commonly referred to as concept searches
```{r}
library(lsa)
#Create term-document matrix (lsa expects a TDM rather than a DTM)
tdm2 <- TermDocumentMatrix(clean.docs)
#summary
tdm2
#inspect segment of document term matrix
inspect(tdm2[1000:1006,1:10])

#what kind of object is the tdm?
class(tdm2)
#note: a simple triplet matrix (STM) is an efficient format to store
#sparse matrices
#need to convert STM to regular matrix
#convert to regular matrix
tdm.matrix <- as.matrix(tdm2)
#check class
class(tdm.matrix)
dim(tdm.matrix)
```

```{r}
#weight terms and docs

#Note: We weight the TDM before calculating the latent semantic space.
#This is to better reflect the relative importance of each term/doc 
#in relation to the entire corpus (much like tf-idf weighting).  
#It is convenient to express the transformation as a product of 
#two numbers - local and global weight functions, like so:
#a (i,j) = L(i,j)*G(i).
#The local weight function L(i,j) presents the weight of term i 
#in document j. The global weight function G(i) is used to express 
#the weight of the term iacross the entire document set. 

#We'll use the equivalent of tf-idf (local weight - tf, global -idf)
#check out other weighting schemes in the documentation (link above)
tdm.matrix.lsa <- lw_tf(tdm.matrix) * gw_idf(tdm.matrix)
dim(tdm.matrix.lsa)

```
##LDA
In natural language processing, latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar.

Every document is a mixture of topics. We imagine that each document may contain words from several topics in particular proportions. For example, in a two-topic model we could say “Document 1 is 90% topic A and 10% topic B, while Document 2 is 30% topic A and 70% topic B.”
Every topic is a mixture of words. For example, we could imagine a two-topic model of American news, with one topic for “politics” and one for “entertainment.” The most common words in the politics topic might be “President”, “Congress”, and “government”, while the entertainment topic may be made up of words such as “movies”, “television”, and “actor”. Importantly, words can be shared between topics; a word like “budget” might appear in both equally.
LDA is a mathematical method for estimating both of these at the same time: finding the mixture of words that is associated with each topic, while also determining the mixture of topics that describes each document. There are a number of existing implementations of this algorithm, and we’ll explore one of them in depth.


```{r}
#compute the Latent semantic space
lsaSpace <- lsa(tdm.matrix.lsa, dimcalc_share()) # create LSA space
#examine output
names(lsaSpace)
```
```{r}
#Original Matrix is decomposed as: 
#tk(nterms,lsadim).Sk(lsadim).dk*(lsadim,ndocs)
#where 
#nterms=number of terms in TDM
#ndocs=number of docs in TDM
#lsadim=dimensionality of Latent Semantic Space (length of Sk)
LSAMat <- as.textmatrix(lsaSpace)
#1)
#Examine a term in LS space
LSAMat["risk",1:41]
#compare to Term-frequency space
tdm.matrix.lsa["project",1:41]
#plot risk frequency

```
```{r}
LSAMat <- as.textmatrix(lsaSpace)
#1)
#Examine a term in LS space
LSAMat["eleph",1:41]
#compare to Term-frequency space
tdm.matrix.lsa["amok",1:41]
#plot risk frequency

```

```{r}
LSAMat <- as.textmatrix(lsaSpace)
#1)
#Examine a term in LS space
LSAMat["model",1:41]
#compare to Term-frequency space
tdm.matrix.lsa["carlo",1:41]
#plot risk frequency
```

```{r}
 #create a two topic model
 # set a seed so that the output of the model is predictable
data_lda <- LDA(tdm2, k = 2, control = list(seed = 1234))
data_lda
```
```{r}
#Besides estimating each topic as a mixture of words, LDA also models each document as a mixture of topics. We can examine the per-document-per-topic probabilities, called  
lda_topics <-tidy(tdm2, matrix = "beta")
lda_topics
#BETA NOT SHOWING UP? WHY NOT - reference is here https://www.tidytextmining.com/topicmodeling.html
```
```{r}
#order to get the top terms
termlda <- "term"
documentlda <- "document"
lda_top_terms <- lda_topics %>%
  group_by(termlda) %>%
  top_n(100, beta) %>%
  ungroup() %>%
  arrange(termlda, -beta)
```

