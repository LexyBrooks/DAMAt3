---
title: "AT3_consolidated"
author: "Alex Brooks"
date: "27/10/2018"
output: html_document
---

##My goal is to find out what's in these 41 different text documents
```{r setup, include=FALSE}
library(tm) 
library(lsa)
library(SnowballC)
library(ggplot2)
library(wordcloud)
library(cluster)
library(dplyr)      #manipulate dataframes
library(knitr)       # used to make kable tables
library(magrittr)
library(ggthemes)    #to make plots look more interesting
library(topicmodels)
library(tidytext)
library(tidyr)
library(kableExtra)
library(igraph)
library(ggraph)
library(widyr)
```

Looking at docs corpus and analysing themes means loading the docs and cleaning them to turn them into the correct format for analysis.

```{r}
#Load the corpus
docs <- VCorpus(DirSource("../raw_data/doc_corpus"))

num.docs <- length(docs)

```

#Write a standard function to clean the documents
```{r}

clean_docs <- function(docs) {
  
  output = docs
  
  #Remove punctuation - replace punctuation marks with " "
  output = tm_map(output, removePunctuation)
  
  #Transform to lower case
  output = tm_map(output,content_transformer(tolower))
  
  #Strip digits
  output = tm_map(output, removeNumbers)
  
  #Remove stopwords from standard stopword list 
  output = tm_map(output, removeWords, stopwords("english"))
  
  #Create a custom set of stopwords that weren't removed by the standard stopwords list
  myStopwords <- c("can",
                   "also",
                   "get",
                   "see",
                   "may",
                   "much",
                   "now",
                   "said",
                   "will",
                   "way",
                   "well",
                   "howev",
                   "say",
                   "one",
                   "use",
                   "tm_map",
                   "t_")
  
  #Remove the custom stopwords
  output = tm_map(output, removeWords, myStopwords)
  
  #Strip whitespace (cosmetic?)
  output = tm_map(output, stripWhitespace)
  
  return(output)
  
}
```

#A bit of basic analysis of documents with this function
```{r}

document_info <- function(documents, clean.documents) {
  
  num.documents = length(clean.documents)
  
  for(i in 1:num.documents) {
    document.meta = clean.documents[[i]]$meta
    document.content = clean.documents[[i]]$content
    raw.document.content = documents[[i]]$content
    
    if (i == 1) {
      document.summary = data.frame(id = document.meta$id,
                                    language = document.meta$language,
                                    datetimestamp = document.meta$datetimestamp,
                                    raw.characters = nchar(raw.document.content),
                                    clean.characters = nchar(document.content))
    } else {
      document.temp = data.frame(id = document.meta$id,
                                 language = document.meta$language,
                                 datetimestamp = document.meta$datetimestamp,
                                 raw.characters = nchar(raw.document.content),
                                 clean.characters = nchar(document.content))
      
      document.summary = rbind(document.summary, document.temp)
    }
  }
  
  return(document.summary)
}
```

#A function to put document text into a table if needed
```{r}

document_table <- function(documents) {
  for (i in 1:length(documents)) {
    document.meta = documents[[i]]$meta
    document.content = documents[[i]]$content
    
    if (i == 1) {
      document.summary = data.frame(id = document.meta$id,
                                    text = document.content)
    } else {
      document.temp = data.frame(id = document.meta$id,
                                 text = document.content)
      
      document.summary = rbind(document.summary, document.temp)
    }
  }
  
  return(document.summary)
}
```

##Clean the documents and create a variable with stems of the cleaned corpus
```{r}

clean.docs <- clean_docs(docs)

stem.clean.docs <- tm_map(clean.docs,stemDocument)

```

```{r}
#running the function on raw docs and clean docs
doc.info <- document_info(docs, clean.docs)

#View the raw character length of each document
ggplot(doc.info, aes(reorder(id,raw.characters), raw.characters)) +
    geom_bar(stat = "identity", fill='lightblue', color='black') +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

##Create a document term matrix and a term document matrix

```{r}
#Create document-term matrix
doc.term.matrix <- DocumentTermMatrix(stem.clean.docs)
term.doc.matrix <- TermDocumentMatrix(stem.clean.docs)

```


```{r}
#Get the list of terms ordered by use of that term within the docs
doc.terms <- as.matrix(doc.term.matrix)
doc.term.freq <- colSums(doc.terms)

```

```{r}
#create a data frame with terms and their frequency
df.term.freq <- as.data.frame(doc.term.freq)
#convert ronames (terms) to a column of the data frame
df.term.freq$terms <- rownames(df.term.freq)
#change the column name of frequency
colnames(df.term.freq)[names(df.term.freq) == "doc.term.freq"] <- "freq"
#remove rownames
rownames(df.term.freq) <- NULL
#order by frequenecy
df.term.freq <- df.term.freq[order(df.term.freq$freq, decreasing=TRUE),]
```

```{r}
#View terms with a frequency greater than 100
ggplot(subset(df.term.freq, freq>100), aes(reorder(terms,freq), freq)) +
    geom_bar(stat = "identity", fill='lightblue', color='black') +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

```{r}
#Let's make a word cloud of the top 50 terms by frequency
set.seed(42)
wordcloud(df.term.freq$terms,df.term.freq$freq,max.words=50,colors=brewer.pal(6,"Dark2"))
```

#Making bigrams of words in the corpus

```{r}
bigram.tokenizer <-  function(x) {
    unlist(
        lapply(
            ngrams(words(x), 2),
            paste,
            collapse = " "),
        use.names = FALSE)
}

bigram.doc.term.matrix <- DocumentTermMatrix(stem.clean.docs, control = list(tokenize = bigram.tokenizer))
bigram.doc.term.freq <- colSums(as.matrix(bigram.doc.term.matrix))

#create a data frame with terms and their frequency
df.bi.freq <- as.data.frame(bigram.doc.term.freq)
#convert rownames (terms) to a column of the data frame
df.bi.freq$terms <- rownames(df.bi.freq)
#change the column name of frequency
colnames(df.bi.freq)[names(df.bi.freq) == "bigram.doc.term.freq"] <- "freq"
#remove rownames
rownames(df.bi.freq) <- NULL
#order by frequenecy
df.bi.freq <- df.bi.freq[order(df.bi.freq$freq, decreasing=TRUE),]
```

```{r}
trigram.tokenizer <-  function(x) {
    unlist(
        lapply(
            ngrams(words(x), 3),
            paste,
            collapse = " "),
        use.names = FALSE)
}

trigram.doc.term.matrix <- DocumentTermMatrix(stem.clean.docs, control = list(tokenize = trigram.tokenizer))
trigram.doc.term.freq <- colSums(as.matrix(trigram.doc.term.matrix))

#create a data frame with terms and their frequency
df.tri.freq <- as.data.frame(trigram.doc.term.freq)
#convert ronames (terms) to a column of the data frame
df.tri.freq$terms <- rownames(df.tri.freq)
#change the column name of frequency
colnames(df.tri.freq)[names(df.tri.freq) == "trigram.doc.term.freq"] <- "freq"
#remove rownames
rownames(df.tri.freq) <- NULL
#order by frequenecy
df.tri.freq <- df.tri.freq[order(df.tri.freq$freq, decreasing=TRUE),]

```

#Create a data frame of each document name and their corresponding text
```{r}
document.text <- document_table(stem.clean.docs)
```

```{r}
#trying with tidytext
corpus_bigrams <- unnest_tokens(document.text, bigram, text, token = "ngrams", n = 2)
```

```{r}
#counting bigrams
bigrams_separated <- corpus_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts <- bigrams_filtered %>%
  count(word1, word2, sort = TRUE)

bigram_graph <- bigram_counts %>%
  filter(n > 20) %>%
  graph_from_data_frame()

set.seed(42)

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)

```


```{r, fig.width=12}
#improving the visual
arrow <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = arrow, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void() + labs(title = "Most frequently occuring bigrams")
```

```{r}
#tidytext approach first - which documents does management appear in? Only 25 (? or 34) of the 41 docs
docs.with.management <- bigrams_separated %>%
  filter(word1 == "manag") %>%
  count(id, word1, sort = TRUE)

ggplot(docs.with.management, aes(reorder(id,n), n)) +
    geom_bar(stat = "identity", fill='lightblue', color='black') +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
```



 
```{r}
docs.with.proj.mgmt <- corpus_bigrams %>%
  filter(bigram =="project manag") %>%
  count(id, bigram, sort = TRUE)

ggplot(docs.with.proj.mgmt, aes(reorder(id,n), n)) +
    geom_bar(stat = "identity", fill='lightblue', color='black') +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
```


```{r}
docs.with.risk.mgmt <- corpus_bigrams %>%
  filter(bigram =="risk manag") %>%
  count(id, bigram, sort = TRUE)

ggplot(docs.with.risk.mgmt, aes(reorder(id,n), n)) +
    geom_bar(stat = "identity", fill='lightblue', color='black') +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
```


```{r}
docs.with.monte.carlo <- corpus_bigrams %>%
  filter(bigram =="mont carlo") %>%
  count(id, bigram, sort = TRUE)

ggplot(docs.with.monte.carlo, aes(reorder(id,n), n)) +
    geom_bar(stat = "identity", fill='lightblue', color='black') +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

#Let's look at Term Frequency but using inverse document frequency
Note that the weighting is normalised by default (that is, the term frequencies in a document are normalised by the number of terms in the document)

```{r}
idf.doc.term.matrix <- DocumentTermMatrix(stem.clean.docs, control = list(weighting = weightTfIdf))
```

```{r}
#Get the list of terms ordered by use of that term within the docs
idf.doc.terms <- as.matrix(idf.doc.term.matrix)
idf.doc.term.freq <- colSums(idf.doc.terms)

```

```{r}
#create a data frame with terms and their frequency
df.idf.term.freq <- as.data.frame(idf.doc.term.freq)
#convert ronames (terms) to a column of the data frame
df.idf.term.freq$terms <- rownames(df.idf.term.freq)
#change the column name of frequency
colnames(df.idf.term.freq)[names(df.idf.term.freq) == "idf.doc.term.freq"] <- "freq"
#remove rownames
rownames(df.idf.term.freq) <- NULL
#order by frequenecy
df.idf.term.freq <- df.idf.term.freq[order(df.idf.term.freq$freq, decreasing=TRUE),]

View(df.idf.term.freq)

```

```{r}
#View terms with a frequency greater than 0.1
ggplot(subset(df.idf.term.freq, freq>0.1), aes(reorder(terms,freq), freq)) +
    geom_bar(stat = "identity", fill='lightblue', color='black') +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

#wordcloud
```{r}
set.seed(42)
#limit words by specifying min total wt
wordcloud(df.idf.term.freq$terms,df.idf.term.freq$freq, max.words=50,colors=brewer.pal(6,"Dark2"))
```


#LSA
Also called latent semantic analysis (LSA), uncovers the underlying latent semantic structure in the usage of words in a body of text and how it can be used to extract the meaning of the text in response to user queries, commonly referred to as concept searches


```{r}
#Use term-document matrix (lsa expects a TDM rather than a DTM) and convert to regular matrix
lsa.matrix <- as.matrix(term.doc.matrix)
```


###weight terms and docs

Note: We weight the TDM before calculating the latent semantic space. This is to better reflect the relative importance of each term/doc in relation to the entire corpus (much like tf-idf weighting). It is convenient to express the transformation as a product of two numbers - local and global weight functions, like so: a (i,j) = L(i,j)*G(i).

The local weight function L(i,j) presents the weight of term i in document j. The global weight function G(i) is used to express the weight of the term iacross the entire document set. 

We'll use the equivalent of tf-idf (local weight - tf, global -idf).

```{r}
lsa.weighted.matrix <- lw_tf(lsa.matrix) * gw_idf(lsa.matrix)
```


##LDA
In natural language processing, latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar.

Every document is a mixture of topics. We imagine that each document may contain words from several topics in particular proportions. For example, in a two-topic model we could say “Document 1 is 90% topic A and 10% topic B, while Document 2 is 30% topic A and 70% topic B.”
Every topic is a mixture of words. For example, we could imagine a two-topic model of American news, with one topic for “politics” and one for “entertainment.” The most common words in the politics topic might be “President”, “Congress”, and “government”, while the entertainment topic may be made up of words such as “movies”, “television”, and “actor”. Importantly, words can be shared between topics; a word like “budget” might appear in both equally.
LDA is a mathematical method for estimating both of these at the same time: finding the mixture of words that is associated with each topic, while also determining the mixture of topics that describes each document. There are a number of existing implementations of this algorithm, and we’ll explore one of them in depth.


```{r}
#compute the Latent semantic space
lsaSpace <- lsa(lsa.weighted.matrix, dimcalc_share()) # create LSA space
#examine output
names(lsaSpace)
```
```{r}
#Original Matrix is decomposed as: 
#tk(nterms,lsadim).Sk(lsadim).dk*(lsadim,ndocs)
#where 
#nterms=number of terms in TDM
#ndocs=number of docs in TDM
#lsadim=dimensionality of Latent Semantic Space (length of Sk)
LSAMat <- as.textmatrix(lsaSpace)
#1)
#Examine a term in LS space
LSAMat["risk",1:41]
#compare to Term-frequency space
lsa.weighted.matrix["risk",1:41]
#plot risk frequency

```
```{r}
LSAMat <- as.textmatrix(lsaSpace)
#1)
#Examine a term in LS space
LSAMat["eleph",1:41]
#compare to Term-frequency space
lsa.weighted.matrix["amok",1:41]
#plot risk frequency

```

```{r}
LSAMat <- as.textmatrix(lsaSpace)
#1)
#Examine a term in LS space
LSAMat["model",1:41]
#compare to Term-frequency space
lsa.weighted.matrix["carlo",1:41]
#plot risk frequency
```

```{r}
 #create a two topic model
 # set a seed so that the output of the model is predictable
data_lda <- LDA(doc.term.matrix, k = 3, control = list(seed = 1234))

```

```{r}
#Besides estimating each topic as a mixture of words, LDA also models each document as a mixture of topics. We can examine the per-document-per-topic probabilities, called  

lda_topics <-tidy(data_lda, matrix = "beta")
lda_topics
#BETA NOT SHOWING UP? WHY NOT - reference is here https://www.tidytextmining.com/topicmodeling.html
```


```{r}
#order to get the top terms
lda_top_terms <- lda_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)
```

```{r}
lda_top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```

```{r, fig.width=10, fig.height=5}

document.text$text <- as.character(document.text$text)

# split into words
by_chapter_word <- document.text %>%
  unnest_tokens(word, text)

# find document-word counts
word_counts <- by_chapter_word %>%
  anti_join(stop_words) %>%
  count(id, word, sort = TRUE) %>%
  ungroup()

#Cast to a document term matrix
docs.dtm <- word_counts %>%
  cast_dtm(id, word, n)

#Do LDA
docs.lda <- LDA(docs.dtm, k = 3, control = list(seed = 1234))

#Get per-topic term probabilities
docs.topics <- tidy(docs.lda, matrix = "beta")

#Find the top 10 topic terms
doc.top.terms <- docs.topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)
```

```{r}
#Plot the top 10 terms for each topic
doc.top.terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```

```{r}
#Examine the per-document-per-topic probabilities
docs.gamma <- tidy(docs.lda, matrix = "gamma")

#PLot the probabilities
docs.gamma %>%
  mutate(document = reorder(document, gamma * topic)) %>%
  ggplot(aes(factor(topic), gamma)) +
  geom_boxplot() +
  facet_wrap(~ document)

```

```{r}
docs.gamma$gamma.integer <- round(docs.gamma$gamma * 10)

docs.gamma %>%
  mutate(document = reorder(document, gamma * topic))
```

```{r}

docs.gamma %>%
  filter(gamma.integer > 7, topic == 1)

```

```{r}
docs.gamma %>%
  filter(gamma.integer > 7, topic == 2)
```



```{r}
docs.gamma %>%
  filter(gamma.integer > 7, topic == 3)
```


```{r}
docs.gamma %>%
  filter((gamma.integer < 7 & topic == 1) & (gamma.integer < 7 & topic == 2) & (gamma.integer < 7 & topic == 3))
```

```{r}
  for (i in 1:num.docs) {
    if (i < 10) {
      doc.name = paste("Doc0", i, ".txt", sep="")
    } else {
      doc.name = paste("Doc", i, ".txt", sep="")
    }
    temp.gamma <- docs.gamma %>%
      filter(document == doc.name)
    
      topic1 = temp.gamma$gamma[temp.gamma$topic == 1]
      topic1.int = temp.gamma$gamma.integer[temp.gamma$topic == 1]
      topic2 = temp.gamma$gamma[temp.gamma$topic == 2]
      topic2.int = temp.gamma$gamma.integer[temp.gamma$topic == 2]
      topic3 = temp.gamma$gamma[temp.gamma$topic == 3]
      topic3.int = temp.gamma$gamma.integer[temp.gamma$topic == 3]
      
      temp = data.frame(document=doc.name, topic1.gamma = topic1, topic2.gamma = topic2, topic3.gamma = topic3, topic1.gamma.int = topic1.int, topic2.gamma.int = topic2.int, topic3.gamma.int = topic3.int)
      
      if (i == 1) {
        new.df = temp
      } else {
        new.df = rbind(new.df, temp)
      }
  }

new.df %>%
  filter(topic1.gamma.int <= 7, topic2.gamma.int <= 7, topic3.gamma.int <= 7)
```